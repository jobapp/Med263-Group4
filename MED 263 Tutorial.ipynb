{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genetic mutations are thought to be the main cause of cancer. These functional changes in protein products result in cancers that may have radically different behaviors in terms of disease progression and therefore treatment options. Genes that are known to be mutated in breast cancers include BRCA1 and 2, TP53, PIK3CA, HER2. However, there are hundreds of other genes whose role in breast cancer is yet to be fully understood, including those which influence the expression levels of a gene which may otherwise be normal. RNA Seq is a next-generation sequencing (NGS) tool which allows for the quantitative measure of gene expression. This coupled with gene set enrichment analysis can allow for physicians and researchers to better understand the complexities within patients’ cancer and lead to more effective therapies.\n",
    "\n",
    "In this exercise, we will be looking at a breast cancer data set from TCGA (BRCA cohort) consisting of 1097 different patients that have had their gene expression quantified with RNAseq and their somatic mutation profile assessed by NGS. We will be performing dimensionality reduction via non-negative matrix factorization (NMF) in order to reduce the complex TCGA dataset to two |W| x |H|, for genes and patient ID, respectively. Patients were then reorganized into a z-normalized, hierarchically-clustered heatmap, with 11 clusters identified with 1 to 465 patients per group. Kaplan-Meier curves comparing survivability of various clusters. was then performed. We will be using the patients’ RNAseq expression as a readout for cancer type to cluster them into functional groups.\n",
    "The resulting groups of this unsupervised clustering will then be annotated using ssGSEA (single sample GSEA) in order to assign biological meaning to the different groups of samples. We will also be examining which specific mutations are more associated with certain groups to determine the “root cause” of the observed cancer expression pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Download Software and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial uses Python 3 and the Jupyter notebook environment. You may already have these on your system.\n",
    "\n",
    "Install Python 3: https://www.python.org/downloads/\n",
    "\n",
    "You may have to install pip separately on some systems.\n",
    "\n",
    "Then install Jupyter Notebook: https://jupyter.org/install\n",
    "\n",
    "Test if your install was successful by opening a terminal and running one of these:\n",
    "```python3 -m notebook```\n",
    "\n",
    "```py -3.X -m notebook```\n",
    "\n",
    "X = your version of python, for sample Python X = 7 for Python 3.7\n",
    "\n",
    "```jupyter notebook```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Packages:\n",
    "### Pandas\n",
    "  [Pandas](https://pandas.pydata.org/) is a data analytics tool built on python which we will use to import, visualize and clean our data. \n",
    "### NumPy\n",
    "  [NumPy](https://scipy.org/install/) is a mathematical library optimized for very fast calculations.\n",
    "### SciPy\n",
    "  [SciPy](https://scipy.org/install/) is a scientific computing library that we use for hierarchical clustering.  \n",
    "### MatplotLib\n",
    "  [MatplotLib](https://matplotlib.org/stable/users/installing/index.html) is a library for plotting graphs and other basic visualization functionality. \n",
    "### Seaborn \n",
    " [Seaborn](https://seaborn.pydata.org/) is what we will use to visualize our data. \n",
    "### Lifelines\n",
    "  [Lifelines](https://github.com/CamDavidsonPilon/lifelines/) is a survival analysis library used to create Kaplan-Meier survival plots. \n",
    "\n",
    "  \n",
    "### sklearn\n",
    "  [ScikitLearn](https://scikit-learn.org/stable/install.html) is a machine learning library.  Here we use it to perform Non-Negative Matrix Factorization (NMF).\n",
    "### StatsModels\n",
    "  [statsmodels](https://www.statsmodels.org/dev/install.html) is a statistical package for doing a variety of data analysis and statistics.  Here it is used for False Discovery Rate (FDR) p-value correction.  \n",
    "  \n",
    "All of the above packages can be installed with pip, anaconda, or whatever other package/environment manager you prefer.  You may already have some or all of these installed.  Pip commands are given here:\n",
    "```\n",
    "pip install -U pandas numpy scipy matplotlib seaborn lifelines scikit-learn statsmodels\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage,fcluster,cut_tree,set_link_color_palette\n",
    "from scipy.stats import mannwhitneyu,norm,sem\n",
    "\n",
    "import sklearn.decomposition\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm,colors\n",
    "import itertools\n",
    "\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "import sys\n",
    "from ssGSEA import single_sample_gseas\n",
    "\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above import commands do not result in an error, then your installs were successful.  This was also the first piece of code needed for the tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data:\n",
    "Data for this tutorial comes from the Breast Cancer (BRCA) Cohort of [The Cancer Genome Atlas](https://portal.gdc.cancer.gov/projects/TCGA-BRCA).\n",
    "\n",
    "Clinical Data: https://docs.google.com/spreadsheets/d/1dpBjMe0RNiGxcJWYNHcMOBDDSTGDzQJmNq8C_4Bd_4E/edit?usp=sharing\n",
    "\n",
    "Expression Data: https://drive.google.com/file/d/1MU4dM7mpBTy933Nx5jNAzVZ8y1EaZ7T0/view?usp=sharing\n",
    "\n",
    "Gene Sets (Already in this repository in the [data directory](data/MSigDB_breast_cancer_subtypes_gene_sets.gmt)): https://drive.google.com/file/d/1-BA3hxGLmQhFs77b8Hno9N4_FuEUXLIW/view?usp=sharing\n",
    "\n",
    "Place all downloaded data into the [data directory](data/).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Cleanup\n",
    "  \n",
    "  \n",
    "First we will want to import the gene expression for each patient as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expression_df = pd.read_table(\"./data/TCGA_BRCA_EXP.v1.gct\",index_col=0,skiprows=2)\n",
    "expression_df = expression_df[[c for c in expression_df.columns if c !=\"Description\"]]\n",
    "expression_df = expression_df.rename(columns={c:c.replace(\"_\",\"-\") for c in expression_df.columns})\n",
    "expression_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can import the patient's clininically relevant data into a seperate pandas table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clinical_info_df = pd.read_csv(\"./data/TCGA_BRCA_clinical_FH.csv\",index_col=0)\n",
    "clinical_info_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TCGA data was collected from many different institutions (\"sites\") nationwide, and each has slightly different ways to record clinical data.  This results in a very \"scattered\" data table.  For example, you can see here that the time of event stored in two different columns depending on whether the patient is dead or alive.  There are also patients who have timepoint values recorded in both, or the wrong column, so you cannot just \"pick one\" or else you will lose info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clinical_info_df[[\"days_to_last_followup\",'days_to_death', 'vital_status']] \n",
    "df.loc[df[\"vital_status\"]==\"Dead\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code below \"regularizes\" the timepoint data into one column by choosing the approrpiate column depending on the patient's vital status (Dead/Alive).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"days_to_last_followup\"] = pd.to_numeric(df[\"days_to_last_followup\"],errors=\"coerce\")\n",
    "df[\"days_to_death\"] = pd.to_numeric(df[\"days_to_death\"],errors=\"coerce\")\n",
    "\n",
    "vital_status_df_dict = {\"sample\":[],\"event_time\":[],\"vital_status\":[]}\n",
    "for index,row in df.iterrows():\n",
    "    timepoint = 0\n",
    "    if row[\"vital_status\"] ==\"Alive\":\n",
    "        if row.isnull()[\"days_to_last_followup\"]==False:\n",
    "            vital_status_df_dict[\"sample\"].append(index)\n",
    "            vital_status_df_dict[\"event_time\"].append(row[\"days_to_last_followup\"])\n",
    "            vital_status_df_dict[\"vital_status\"].append(row[\"vital_status\"])\n",
    "        elif row.isnull()[\"days_to_death\"]==False:\n",
    "            vital_status_df_dict[\"sample\"].append(index)\n",
    "            vital_status_df_dict[\"event_time\"].append(row[\"days_to_death\"])\n",
    "            vital_status_df_dict[\"vital_status\"].append(row[\"vital_status\"])\n",
    "    elif row[\"vital_status\"] ==\"Dead\":\n",
    "        if row.isnull()[\"days_to_death\"]==False:\n",
    "            vital_status_df_dict[\"sample\"].append(index)\n",
    "            vital_status_df_dict[\"event_time\"].append(row[\"days_to_death\"])\n",
    "            vital_status_df_dict[\"vital_status\"].append(row[\"vital_status\"])\n",
    "        elif row.isnull()[\"days_to_last_followup\"]==False:\n",
    "            vital_status_df_dict[\"sample\"].append(index)\n",
    "            vital_status_df_dict[\"event_time\"].append(row[\"days_to_last_followup\"])\n",
    "            vital_status_df_dict[\"vital_status\"].append(row[\"vital_status\"])\n",
    "vital_status_df = pd.DataFrame(vital_status_df_dict).set_index(\"sample\")\n",
    "vital_status_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now cleaned, even for dead patients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital_status_df.loc[vital_status_df[\"vital_status\"]==\"Dead\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Dimensionality Reduction\n",
    "Dimensionality Reduction is a means of transforming highly dimensional data (like TCGA data) to a lower dimension data set, or matrix, that can more easily be analyzed. The method we will be using is Non-Negative Matrix Factorization (NMF) which will transform/approximate our original dataset |V| into two matrices,|W| and |H|, such that **W x H = V** plus an error matrix that is not used. We will also z-normalize the outputted matrices so that we can more easily interpret and visualize the data.\n",
    "  \n",
    "This code creates the NMF decomposition and z-normalize functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMF_decomposition(data,n_comp,max_iter=1000):\n",
    "    model = sklearn.decomposition.NMF(n_components = n_comp,\n",
    "                                      init = 'nndsvdar',\n",
    "                                      solver = 'cd',\n",
    "                                      max_iter = max_iter,\n",
    "                                      tol = 1e-10,\n",
    "                                      random_state = 12345, shuffle = False, verbose = False)\n",
    "    w = pd.DataFrame(model.fit_transform(data + 0.001), index = data.index, columns = ['F{}'.format(x) for x in range(n_comp)])\n",
    "    h = pd.DataFrame(model.components_, index = ['F{}'.format(x) for x in range(n_comp)], columns = data.columns)\n",
    "    return w,h\n",
    "\n",
    "def z_normalize_group(exp_df_in,do_clip = False,do_shift = False,do_rank = False):\n",
    "    exp_df_in_norm = exp_df_in.copy()\n",
    "    exp_df_in_norm = exp_df_in_norm[~exp_df_in_norm.index.duplicated(keep='first')]\n",
    "    if do_rank==True:\n",
    "        exp_df_in_norm = exp_df_in_norm.rank(axis=0, method='average', numeric_only=None, na_option='keep', ascending=True, pct=False)\n",
    "    exp_df_in_means = exp_df_in_norm.mean(axis=1)\n",
    "    exp_df_in_stds = exp_df_in_norm.std(axis=1)\n",
    "    for i in exp_df_in_norm.index:\n",
    "        #print(exp_df_in_norm.loc[i,:])\n",
    "        exp_df_in_norm.loc[i,:] = (exp_df_in_norm.loc[i,:] - exp_df_in_means.loc[i])/exp_df_in_stds.loc[i]\n",
    "    if do_clip==True:\n",
    "        exp_df_in_norm.clip(-2, 2, inplace=True) \n",
    "    if do_shift==True:\n",
    "        exp_df_in_norm = exp_df_in_norm + 2\n",
    "    return exp_df_in_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll rank normalize the expression data and perform dimensionality reduction using the functions created above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_expression_df = expression_df.rank(axis=0, method='dense', numeric_only=None, na_option='keep', \n",
    "                           ascending=True, pct=False)\n",
    "W_df,H_df = NMF_decomposition(normalized_expression_df,10,max_iter=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot heatmaps of the resulting W and H using seaborn where W contains patient sample ID, and H contains the gene expression data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,12))\n",
    "sns.heatmap(z_normalize_group(H_df),cmap=\"bwr\",vmin=-2,vmax=2,center=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,12))\n",
    "sns.heatmap(z_normalize_group(W_df),cmap=\"bwr\",vmin=-2,vmax=2,center=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_row_linkage_obj = linkage(distance.pdist(W_df), method='average')\n",
    "W_col_linkage_obj = linkage(distance.pdist(W_df.T), method='average')\n",
    "\n",
    "H_row_linkage_obj = linkage(distance.pdist(H_df), method='average')\n",
    "H_col_linkage_obj = linkage(distance.pdist(H_df.T), method='average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster of genes (**W**):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(W_df,\n",
    "               row_linkage=W_row_linkage_obj,\n",
    "               col_linkage = W_col_linkage_obj,\n",
    "               figsize=(15,20),\n",
    "               #square=True,\n",
    "               z_score=0,\n",
    "               center=0,\n",
    "               vmin=-2,\n",
    "               vmax=2,\n",
    "               cmap=\"bwr\"\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster of Patients (**H**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.clustermap(H_df,\n",
    "               row_linkage=H_row_linkage_obj,\n",
    "               col_linkage = H_col_linkage_obj,\n",
    "               figsize=(15,15),\n",
    "               #square=True,\n",
    "               z_score=0,\n",
    "               center=0,\n",
    "               vmin=-2,\n",
    "               vmax=2,\n",
    "               cmap=\"bwr\"\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have clusters, it is always nice to visualize them using color:\n",
    "This step is also necessary to make sure future plots of multiple clusters have the same colo/cluster assignments as this first dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap_hex = []\n",
    "colormap_obj = cm.get_cmap('Paired')\n",
    "for i in range(0,colormap_obj.N):\n",
    "    colormap_hex.append(colors.rgb2hex(colormap_obj(i)))\n",
    "colormap_hex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "height_threshold = 100\n",
    "plt.figure(figsize=(20, 12))\n",
    "set_link_color_palette(colormap_hex)\n",
    "\n",
    "\n",
    "#f, (ax1, ax2) = plt.subplots(2,1,sharex=True,figsize=(15,12), constrained_layout=True)\n",
    "dendrogram_dict = dendrogram(H_col_linkage_obj,\n",
    "                             orientation='top',\n",
    "                             #labels=cluster_assignments_series.index,\n",
    "                             distance_sort='descending',\n",
    "                             color_threshold = height_threshold,\n",
    "                             #link_color_func={i:color_palette_lst[i] for i in range(0,len(color_palette_lst))},\n",
    "                             show_leaf_counts= False,\n",
    "                             #ax=ax1\n",
    "                            )\n",
    "#ax2.imshow([cluster_assignments_series.iloc[dendrogram_dict[\"leaves\"]].values]*100, cmap='Set2', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have nice clusters, we can create a dataframe, assigning each patient to a cluster for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluter_assignments_arr = cut_tree(H_col_linkage_obj,height=height_threshold).flatten()[dendrogram_dict[\"leaves\"]]\n",
    "plt.figure(figsize=(20, 12))\n",
    "sns.scatterplot(x=range(0,len(cluter_assignments_arr)),y=cluter_assignments_arr)\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Cluster Number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the dataframe is good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignments_dict = {\"sample\":pd.Series(H_df.columns).iloc[dendrogram_dict[\"leaves\"]].values,\n",
    "                            \"cluster\":cluter_assignments_arr}\n",
    "cluster_assignments_series = pd.DataFrame(cluster_assignments_dict).set_index(\"sample\")[\"cluster\"]\n",
    "cluster_assignments_series = cluster_assignments_series\n",
    "cluster_assignments_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many patients are in each cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignments_series.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Annotate Clusters of Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "def read_gmt(gmt_file_path, drop_description=True):\n",
    "\n",
    "    lines = []\n",
    "\n",
    "    with open(gmt_file_path) as gmt_file:\n",
    "\n",
    "        for line in gmt_file:\n",
    "\n",
    "            split = line.strip().split(sep=\"\\t\")\n",
    "\n",
    "            lines.append(split[:2] + [gene for gene in set(split[2:]) if gene])\n",
    "\n",
    "    df = DataFrame(lines)\n",
    "\n",
    "    df.set_index(0, inplace=True)\n",
    "\n",
    "    df.index.name = \"Gene Set\"\n",
    "\n",
    "    if drop_description:\n",
    "\n",
    "        df.drop(1, axis=1, inplace=True)\n",
    "\n",
    "        df.columns = tuple(\"Gene {}\".format(i) for i in range(0, df.shape[1]))\n",
    "\n",
    "    else:\n",
    "\n",
    "        df.columns = (\"Description\",) + tuple(\n",
    "            \"Gene {}\".format(i) for i in range(0, df.shape[1] - 1)\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "from pandas import concat\n",
    "\n",
    "\n",
    "\n",
    "def read_gmts(gmt_file_paths, sets=None, drop_description=True, collapse=False):\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for gmt_file_path in gmt_file_paths:\n",
    "\n",
    "        dfs.append(read_gmt(gmt_file_path, drop_description=drop_description))\n",
    "\n",
    "    df = concat(dfs, sort=True)\n",
    "\n",
    "    if sets is not None:\n",
    "\n",
    "        df = df.loc[(df.index & sets)].dropna(axis=1, how=\"all\")\n",
    "\n",
    "    if collapse:\n",
    "\n",
    "        return df.unstack().dropna().sort_values().unique()\n",
    "\n",
    "    else:\n",
    "\n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform single sample Gene Set Enrichment Analysis (ssGSEA):\n",
    "This tells how overexpressed certain gene sets are within each individual patient's sample.  The 12 gene sets chosen represent different breast cancer subtypes, though many others are available on MSigDB for pathways and other biological phenomena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MSigDB_breast_cancer_subtypes_gene_sets_df = read_gmt(\"./data/MSigDB_breast_cancer_subtypes_gene_sets.gmt\")\n",
    "MSigDB_breast_cancer_subtypes_gene_sets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below does the actual ssGSEA calculation.  It may take some time to run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCGA_breast_ssGSEA_df = single_sample_gseas(expression_df,\n",
    "                                            MSigDB_breast_cancer_subtypes_gene_sets_df,\n",
    "                                            n_job=4) #change n_job to whatever the number of cores you have on your computer\n",
    "#TCGA_breast_ssGSEA_df.to_csv(\"./data/TCGA_breast_ssGSEA_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 vs all mann whitneys + distribution charts\n",
    "def mann_whitney_cluster_1_vs_others(cluster,cluster_assignment_series, data_df):\n",
    "    cluster_samples = cluster_assignment_series.loc[cluster_assignment_series==cluster].index\n",
    "    other_samples = cluster_assignment_series.loc[cluster_assignment_series!=cluster].index\n",
    "    results_df_dict = {\"gene_set\":[],\n",
    "                       \"cluster_avg\":[],\n",
    "                       \"cluster_95%_CI_lower\":[],\n",
    "                       \"cluster_95%_CI_upper\":[],\n",
    "                       \"others_avg\":[],\n",
    "                       \"others_95%_CI_lower\":[],\n",
    "                       \"others_95%_CI_upper\":[],\n",
    "                       \"mann-whitney_p-value\":[]}\n",
    "    for index,row in data_df.iterrows():\n",
    "        cluster_data = row[cluster_samples].dropna()\n",
    "        other_data = row[other_samples].dropna()\n",
    "        cluster_data_mean = cluster_data.mean()\n",
    "        cluster_data_95_CI = norm.interval(alpha=0.95, loc=cluster_data_mean, scale=sem(cluster_data))\n",
    "        other_data_mean = other_data.mean()\n",
    "        other_data_95_CI = norm.interval(alpha=0.95, loc=other_data_mean, scale=sem(other_data))\n",
    "        mann_whitney_result = mannwhitneyu(cluster_data,other_data)\n",
    "        mann_whitney_pval =  mann_whitney_result.pvalue\n",
    "        results_df_dict[\"gene_set\"].append(index)\n",
    "        results_df_dict[\"cluster_avg\"].append(cluster_data_mean)\n",
    "        results_df_dict[\"cluster_95%_CI_lower\"].append(cluster_data_95_CI[0])\n",
    "        results_df_dict[\"cluster_95%_CI_upper\"].append(cluster_data_95_CI[1])\n",
    "        results_df_dict[\"others_avg\"].append(other_data_mean)\n",
    "        results_df_dict[\"others_95%_CI_lower\"].append(other_data_95_CI[0])\n",
    "        results_df_dict[\"others_95%_CI_upper\"].append(other_data_95_CI[1])\n",
    "        results_df_dict[\"mann-whitney_p-value\"].append(mann_whitney_pval)\n",
    "    results_df = pd.DataFrame(results_df_dict).set_index(\"gene_set\")\n",
    "    results_df[\"group_FDR_corrected_p-value\"] = fdrcorrection(results_df_dict[\"mann-whitney_p-value\"])[1]\n",
    "    results_df[\"group_avg_diff\"] = results_df[\"cluster_avg\"]-results_df[\"others_avg\"]\n",
    "    \n",
    "    return results_df.sort_values(by=\"group_avg_diff\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics on the ssGSEA score distributions for each cluster can be displayed.  Which gene sets, when ordered by the difference between the average group of interest and others rises to the top?  Do they cover the main described subtypes of breast cancer?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1_results_df = mann_whitney_cluster_1_vs_others(1,cluster_assignments_series, TCGA_breast_ssGSEA_df)\n",
    "cluster_1_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_2_results_df = mann_whitney_cluster_1_vs_others(2,cluster_assignments_series, TCGA_breast_ssGSEA_df)\n",
    "cluster_2_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_3_results_df = mann_whitney_cluster_1_vs_others(3,cluster_assignments_series, TCGA_breast_ssGSEA_df)\n",
    "cluster_3_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_4_results_df = mann_whitney_cluster_1_vs_others(4,cluster_assignments_series, TCGA_breast_ssGSEA_df)\n",
    "cluster_4_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_5_results_df = mann_whitney_cluster_1_vs_others(5,cluster_assignments_series, TCGA_breast_ssGSEA_df)\n",
    "cluster_5_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the ```cluster_relabel_dict``` below with your interpretation of what each cluster is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_relabel_dict = {1:\"Cluster 1: \",\n",
    "                        2:\"Cluster 2: \",\n",
    "                        3:\"Cluster 3: \",\n",
    "                        4:\"Cluster 4: \",\n",
    "                        5:\"Cluster 5: \"}\n",
    "#Don't worry about the other clusters.  \n",
    "\n",
    "for cluster_number in cluster_assignments_series.unique():\n",
    "    if cluster_number not in cluster_relabel_dict:\n",
    "        cluster_relabel_dict[cluster_number] = \"Cluster {}\".format(cluster_number)\n",
    "cluster_relabel_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignments_series = cluster_assignments_series.replace(to_replace=cluster_relabel_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Survival Analysis\n",
    "Kaplan-Meier Survival Analysis is a simple tool which incorporates successive probabilities of an event to calculate the overall probability of an event occurring, accounting for right-censored data points due to loss of followup, study ending, etc. In this section, we will be using the lifelines KaplanMeierFitter function to calculate and graph the Kaplan-Meier Curve to compare the survival probabilities over time between all the different clusters of patients we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def KM_plot(cluster_assignments_series,\n",
    "            vital_status_df,\n",
    "            colormap_lst = None,\n",
    "            title_txt = \"KM Plot\",\n",
    "            vital_status_alive_txt = \"Alive\",\n",
    "            vital_status_column = \"vital_status\",\n",
    "            vital_status_time_column = 'event_time',\n",
    "            fig_size = (20,15),):\n",
    "    ax = None\n",
    "    sample_group_medians_dict = {\"sample_group\":[],\"median_survival\":[]}\n",
    "    sample_group_kmf_event_data_dict = dict()\n",
    "    colormap_to_use = None\n",
    "    \n",
    "    for sample_group_name in np.sort(cluster_assignments_series.unique()): #sample_group_dict:\n",
    "        samples_in_group_lst = cluster_assignments_series.loc[cluster_assignments_series==sample_group_name].index\n",
    "        sample_group_count = len(samples_in_group_lst)\n",
    "        time_data = vital_status_df.loc[samples_in_group_lst][vital_status_time_column].values\n",
    "        event_data = vital_status_df.loc[samples_in_group_lst][vital_status_column].values\n",
    "        event_data = np.where(event_data == vital_status_alive_txt, 0, 1)\n",
    "        sample_group_kmf_event_data_dict[sample_group_name] = {\"time_data\":time_data,\"event_data\":event_data}\n",
    "        \n",
    "        kmf = KaplanMeierFitter()\n",
    "        kmf.fit(time_data, event_data, label=\"{} (n={})\".format(sample_group_name,sample_group_count))\n",
    "        sample_group_medians_dict[\"sample_group\"].append(sample_group_name)\n",
    "        sample_group_medians_dict[\"median_survival\"].append(kmf.median_survival_time_)\n",
    "        color_to_use = None\n",
    "        if colormap_lst is not None:\n",
    "            color_to_use = colormap_lst[sample_group_name]\n",
    "        if ax==None:\n",
    "            ax = kmf.plot(show_censors=True, ci_show=False,figsize=fig_size,title=title_txt,color=color_to_use)\n",
    "        else:\n",
    "            ax = kmf.plot(show_censors=True, ci_show=False, ax=ax,color=color_to_use)\n",
    "\n",
    "    sample_group_medians_df = pd.DataFrame(sample_group_medians_dict).set_index(\"sample_group\")\n",
    "    logrank_test_df_dict = {\"cluster_A\":[],\n",
    "                              \"cluster_A_median_survival\":[],\n",
    "                              \"cluster_B\":[],\n",
    "                              \"cluster_B_median_survival\":[],\n",
    "                              \"p-value\":[]} \n",
    "    for cluster_pair in itertools.combinations(sample_group_medians_df.index,2):\n",
    "        logrank_test_df_dict[\"cluster_A\"].append(cluster_pair[0])\n",
    "        logrank_test_df_dict[\"cluster_A_median_survival\"].append(sample_group_medians_df.loc[cluster_pair[0]][\"median_survival\"])\n",
    "        logrank_test_df_dict[\"cluster_B\"].append(cluster_pair[1])\n",
    "        logrank_test_df_dict[\"cluster_B_median_survival\"].append(sample_group_medians_df.loc[cluster_pair[1]][\"median_survival\"])\n",
    "        logrank_test_result = logrank_test(sample_group_kmf_event_data_dict[cluster_pair[0]][\"time_data\"],\n",
    "                                   sample_group_kmf_event_data_dict[cluster_pair[1]][\"time_data\"], \n",
    "                                   event_observed_A=sample_group_kmf_event_data_dict[cluster_pair[0]][\"event_data\"], \n",
    "                                   event_observed_B=sample_group_kmf_event_data_dict[cluster_pair[1]][\"event_data\"])\n",
    "        logrank_test_df_dict[\"p-value\"].append(logrank_test_result.p_value)\n",
    "        ax.set_ylim(0,1.05)\n",
    "        ax.set_xlim(0,)\n",
    "    logrank_test_df = pd.DataFrame(logrank_test_df_dict)\n",
    "    logrank_test_df[\"FDR_corrected_p-value\"] = fdrcorrection(logrank_test_df[\"p-value\"])[1]\n",
    "    return ax, logrank_test_df.sort_values(by=\"FDR_corrected_p-value\",ascending=True), sample_group_medians_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize all clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap_cluster_index_arr = np.array(dendrogram_dict['color_list'])[np.unique(cluter_assignments_arr, return_index=True)[1]]\n",
    "\n",
    "colormap_cluster_index_dict = {cluster_relabel_dict[k]:colormap_cluster_index_arr[k] for k in range(0,len(colormap_cluster_index_arr))}\n",
    "\n",
    "KM_plot_ax, KM_stats_df, KM_medians_df = KM_plot(cluster_assignments_series,vital_status_df,colormap_lst=colormap_cluster_index_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaplan-Meier Survival Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KM_medians_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "KM_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "  1. F. Blows et al. Subtyping of Breast Cancer by Immunohistochemistry to Investigate a Relationship between Subtype and Short and Long Term Survival: A Collaborative Analysis of Data for 10,159 Cases from 12 Studies. PLOS Medicine. 2010.  https://doi.org/10.1371/journal.pmed.1000279\n",
    "  2. https://portal.gdc.cancer.gov/projects/TCGA-BRCA \n",
    "  3. Subramanian A, Tamayo P, et.al. Gene set enrichment analysis: A knowledge-based approach for interpreting genome-wide expression profiles. PNAS. \n",
    "  4. Pereira, B., Chin, SF., Rueda, O. et al. The somatic mutation profiles of 2,433 breast cancers refine their genomic and transcriptomic landscapes. Nat Commun 7, 11479 (2016). https://doi.org/10.1038/ncomms11479"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
